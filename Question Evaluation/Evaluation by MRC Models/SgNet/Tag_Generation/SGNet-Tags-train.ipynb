{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b028610c-d236-4c94-b1c3-929319a0024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.chunk.regexp import RegexpParser\n",
    "#from allennlp.predictors.predictor import Predictor\n",
    "#from allennlp_models import pretrained\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#predictor = pretrained.load_predictor(\"structured-prediction-srl-bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f124f22-7a09-429c-9cb2-1b0a11500a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = json.load(open(\"dev-SQuAD-v2.0.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1335d97-1c3f-45a5-a2f0-d1287cb9f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb7c152-54b9-4f15-92e1-ae674d3df9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"20240402-train_modified_Antonym_allData_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bab1611e-a70b-4fab-a662-1ccf9a830b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpsg(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Perform part-of-speech tagging\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Define a regular expression grammar for NP (noun phrase) chunks\n",
    "    grammar = r\"\"\"\n",
    "        NP: {<DT|JJ|NN.*>+} # Chunk sequences of DT, JJ, NN\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a chunk parser using the defined grammar\n",
    "    chunk_parser = RegexpParser(grammar)\n",
    "    \n",
    "    # Parse the tagged tokens to identify NP chunks\n",
    "    parsed_sentence = chunk_parser.parse(tagged_tokens)\n",
    "    \n",
    "    # Extract spans of NP chunks\n",
    "    hpsg_list_doc = []\n",
    "    for subtree in parsed_sentence.subtrees():\n",
    "        if subtree.label() == 'NP':\n",
    "            # Get the span of the NP chunk\n",
    "            start = subtree.leaves()[0][1]\n",
    "            end = subtree.leaves()[-1][1]\n",
    "            hpsg_list_doc.append((start, end))\n",
    "    hpsg_doc = []\n",
    "    for unit in hpsg_list_doc:\n",
    "        start = hpsg_list_doc.index(unit) + 1\n",
    "        end = start\n",
    "        span_str = f\"({start}, {end})\"\n",
    "        hpsg_doc.append(span_str)\n",
    "    return hpsg_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df549073-4e58-4a70-a24d-4c6ce5f69303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predicates(input_text):\n",
    "    doc = nlp(input_text)\n",
    "    pred_type_que = []\n",
    "    pred_head_que = []\n",
    "\n",
    "    # Iterate over each token in the document\n",
    "    for token in doc:\n",
    "        # Check if the token is a predicate (verb)\n",
    "        if token.pos_ == \"VERB\":\n",
    "            # Add the type of the predicate to pred_type_que\n",
    "            pred_type_que.append(token.dep_)\n",
    "            # Add the index of the head of the predicate to pred_head_que\n",
    "            pred_head_que.append(token.head.i + 1)  # Adding 1 to convert to 1-indexing\n",
    "\n",
    "    return pred_type_que, pred_head_que"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b077f1d-0eec-4776-b1cb-bb8f9f07cf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_input_to_output(context, question_data, id):\n",
    "    output = {\n",
    "        \"tag_rep\": {\n",
    "            \"hpsg_list_doc\": [],\n",
    "            \"srlspan_pred_doc\": [],\n",
    "            \"pred_type_que\": [],\n",
    "            \"pred_head_que\": [],\n",
    "            \"srlspan_pred_que\": [],\n",
    "            \"hpsg_list_que\": [],\n",
    "            \"pred_type_doc\": [],\n",
    "            \"srldep_pred_doc\": [],\n",
    "            \"que_tokens\": question_data.split(),\n",
    "            \"doc_tokens\": [sentence.split() for sentence in context.split('. ')],\n",
    "            \"pred_head_doc\": [],\n",
    "            \"srldep_pred_que\": []\n",
    "        },\n",
    "        \"qas_id\": id\n",
    "    }\n",
    "    \n",
    "    doc_sentences = [sentence.strip() for sentence in context.split('.')]\n",
    "    doc_tokens = [[token.text for token in nlp(sentence)] for sentence in doc_sentences]\n",
    "\n",
    "    # Tokenize the sentence\n",
    "    \n",
    "        \n",
    "    # Populate the output dictionary with the extracted information\n",
    "    output[\"tag_rep\"][\"hpsg_list_doc\"] = hpsg(context)\n",
    "    output[\"tag_rep\"][\"hpsg_list_que\"] = hpsg(question_data)\n",
    "    output[\"tag_rep\"][\"pred_type_doc\"], output[\"tag_rep\"][\"pred_head_doc\"] = generate_predicates(context)\n",
    "    output[\"tag_rep\"][\"pred_type_que\"], output[\"tag_rep\"][\"pred_head_que\"] = generate_predicates(question_data)\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24140006-5647-49dd-9c6e-28fab0a2ddda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 324814/324814 [11:42:06<00:00,  7.71it/s]\n"
     ]
    }
   ],
   "source": [
    "string = \"\"\n",
    "counter = 0\n",
    "from tqdm import tqdm\n",
    "for index, row in tqdm(data.iterrows(),total=data.shape[0]):\n",
    "    curstring = json.dumps(convert_input_to_output(row[\"context\"], row[\"modified question\"], row[\"qid\"]))\n",
    "    string += str(curstring) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d57dcc3-696c-46ed-bd6d-2bb6fdcb20da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "\n",
    "# Assuming 'string' contains JSON data with single quotes\n",
    "# Use regular expressions to replace single quotes outside of string values with double quotes\n",
    "#string = re.sub(r\"(?<!\\\\)'(?!s\\b)\", '\"', string)\n",
    "with open(\"antonym_train_span.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddfe513-196b-4583-b217-97d0bf47531c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
