{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a688028f-fc44-4949-bda0-340bb42fcd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\a1\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "'''import libraries used, json to read the data, \n",
    "randint for randomly generated paragraphs, \n",
    "and pandas to create a dataframe to be written to csv'''\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import json\n",
    "fname = 'train-SQuAD-v2.0'\n",
    "f = open(fname + '.json')\n",
    "#load in data as python dictionary using the json library\n",
    "data = json.load(f)\n",
    "data = data[\"data\"]\n",
    "originals = pd.read_csv(\"train_start.csv\")\n",
    "originals = originals.drop('New_Context', axis=1)\n",
    "originals.drop(originals.columns[originals.columns.str.contains(\n",
    "    'unnamed', case=False)], axis=1, inplace=True)\n",
    "samples = pd.read_csv(\"Negation_train.csv\")\n",
    "roberta = pipeline(\"question-answering\", model=\"deepset/roberta-large-squad2\", device=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8435c007-432f-4c6b-ab5b-df6425fbc75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    final = pd.read_csv(\"Task13P-U-train.csv\").to_dict('records')\n",
    "except:\n",
    "    final = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5cc3852-f6ef-4837-900b-de000da29d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'address_output = \"Task12P-U-train.tsv\"\\nrecord_file = open(address_output, \"w\")  ## this is \\'w\\' = write mode\\ntsv_writer_temp = csv.writer(record_file, delimiter=\\'\\t\\', lineterminator=\\'\\n\\')\\ntsv_writer_temp.writerow([\\'Question_ID\\', \\'Context\\', \\'Original_Question\\', \\'Original_Answer\\', \\'Modified_Question\\', \\'New_Context\\', \\'O-Q model2 answer span\\', \\'O-Q model2 start\\', \\'O-Q model2 end\\', \\'O-Q model2 score\\', \\'O-Q model4 answer span\\', \\'O-Q model4 start\\', \\'O-Q model4 end\\', \\'O-Q model4 score\\', \\'O-Q electra answer span\\', \\'O-Q electra start\\', \\'O-Q electra end\\', \\'O-Q electra score\\', \\'O-Q roberta answer span\\', \\'O-Q roberta start\\', \\'O-Q roberta end\\', \\'O-Q roberta score\\', \\'O-Q albert score\\', \\'P-U model2 answer span\\', \\'P-U model2 start\\', \\'P-U model2 end\\', \\'P-U model2 score\\', \\'P-U model4 answer span\\', \\'P-U model4 start\\', \\'P-U model4 end\\', \\'P-U model4 score\\', \\'P-U electra answer span\\', \\'P-U electra start\\', \\'P-U electra end\\', \\'P-U electra score\\', \\'P-U roberta answer span\\', \\'P-U roberta start\\', \\'P-U roberta end\\', \\'P-U roberta score\\', \\'P-U albert score\\'])\\nrecord_file.close()'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address_output = \"Task13P-U-train.tsv\"\n",
    "record_file = open(address_output, \"w\")  ## this is 'w' = write mode\n",
    "tsv_writer_temp = csv.writer(record_file, delimiter='\\t', lineterminator='\\n')\n",
    "tsv_writer_temp.writerow(['Question_ID', 'Context', 'Original_Question', 'Original_Answer', 'Modified_Question', 'O-Q model2 answer span', 'O-Q model2 start', 'O-Q model2 end', 'O-Q model2 score', 'O-Q model4 answer span', 'O-Q model4 start', 'O-Q model4 end', 'O-Q model4 score', 'O-Q electra answer span', 'O-Q electra start', 'O-Q electra end', 'O-Q electra score', 'O-Q roberta answer span', 'O-Q roberta start', 'O-Q roberta end', 'O-Q roberta score', 'O-Q albert score', 'O-Q albert answer span', 'O-Q sgnet answer span', 'O-Q retro answer span', 'P-U model2 answer span', 'P-U model2 start', 'P-U model2 end', 'P-U model2 score', 'P-U model4 answer span', 'P-U model4 start', 'P-U model4 end', 'P-U model4 score', 'P-U electra answer span', 'P-U electra start', 'P-U electra end', 'P-U electra score', 'P-U roberta answer span', 'P-U roberta start', 'P-U roberta end', 'P-U roberta score', 'P-U albert score'])\n",
    "record_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e204f7e0-8db4-4825-981c-84048ec19bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#odata = json.load(open(\"SGO.json\"))\n",
    "#odata = [json.load(open(\"albert0.json\")), json.load(open(\"SGO.json\")), json.load(open(\"retrospective.json\"))]\n",
    "labels = [\"model2\", \"model4\", \"electra\", \"roberta\", \"sgnet\", \"retro\"]\n",
    "def get_output_models(id, question, context):\n",
    "    tag = \"P-U\"\n",
    "    row = {}\n",
    "        #try:\n",
    "    preds = roberta(question=question, context=context,)\n",
    "    #if(labels[i] != \"albert\"):\n",
    "    row[tag +  \" roberta answer span\"] = preds[\"answer\"]\n",
    "    row[tag + \" roberta start\"] = preds[\"start\"]\n",
    "    row[tag + \" roberta end\"] = preds[\"end\"]\n",
    "    row[tag + \" roberta score\"] = preds[\"score\"]\n",
    "#row[\"P-U \" + labels[i + 1] + \" answer span\"] = odata[id] \n",
    "    #for j in range(0, len(odata)):\n",
    "#        row[\"P-U \" + labels[i + j] + \" answer span\"] = odata[j][id]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffbd2bb7-c3e1-46cc-9599-bd8c34d5e384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(df):\n",
    "    final_df = pd.DataFrame(final)\n",
    "    final_df.to_csv(\"Task13P-U-train.csv\")\n",
    "\n",
    "def append_row(row):\n",
    "    with open(\"Task13P-U-train.tsv\",'a', encoding='utf-8') as f1: ## this is 'a' = apend mode\n",
    "        writer=csv.writer(f1, delimiter='\\t',lineterminator='\\n',)\n",
    "        nrow = []\n",
    "        for k, v in row.items():\n",
    "            nrow.append(v)\n",
    "        writer.writerow(nrow) # These are the value for the new row in df \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51111497-6019-4932-a30f-bf362180f52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Different topics:   0%|                                                                        | 0/442 [00:00<?, ?it/s]\n",
      "Going through paragraphs:   0%|                                                                 | 0/66 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                                                       \u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "Different topics:   0%|▎                                                               | 2/442 [00:00<00:31, 13.95it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/72 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                                                       \u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Different topics:   1%|▌                                                               | 4/442 [00:00<00:34, 12.86it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                                                       \u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/43 [00:00<?, ?it/s]\u001b[A\n",
      "Different topics:   1%|▊                                                               | 6/442 [00:00<00:33, 12.92it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/77 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  57%|███████████████████████████████▍                       | 44/77 [00:00<00:00, 436.45it/s]\u001b[A\n",
      "                                                                                                                       \u001b[A\n",
      "Going through paragraphs:   0%|                                                                | 0/148 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  28%|██████████████▉                                       | 41/148 [00:00<00:00, 400.90it/s]\u001b[A\n",
      "Going through paragraphs:  55%|█████████████████████████████▉                        | 82/148 [00:00<00:00, 404.04it/s]\u001b[A\n",
      "Going through paragraphs:  83%|████████████████████████████████████████████         | 123/148 [00:00<00:00, 389.43it/s]\u001b[A\n",
      "Different topics:   2%|█▏                                                              | 8/442 [00:01<01:07,  6.42it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/62 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  85%|███████████████████████████████████████████████        | 53/62 [00:00<00:00, 517.91it/s]\u001b[A\n",
      "Different topics:   2%|█▎                                                              | 9/442 [00:01<01:04,  6.71it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  67%|█████████████████████████████████████                  | 35/52 [00:00<00:00, 348.86it/s]\u001b[A\n",
      "Different topics:   2%|█▍                                                             | 10/442 [00:01<01:04,  6.69it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  42%|██████████████████████▉                                | 33/79 [00:00<00:00, 328.02it/s]\u001b[A\n",
      "Going through paragraphs:  84%|█████████████████████████████████████████████▉         | 66/79 [00:00<00:00, 327.41it/s]\u001b[A\n",
      "Different topics:   2%|█▌                                                             | 11/442 [00:01<01:15,  5.71it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                | 0/149 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  12%|██████▌                                               | 18/149 [00:00<00:00, 173.48it/s]\u001b[A\n",
      "Going through paragraphs:  41%|██████████████████████                                | 61/149 [00:00<00:00, 320.64it/s]\u001b[A\n",
      "Going through paragraphs:  71%|█████████████████████████████████████▋               | 106/149 [00:00<00:00, 376.83it/s]\u001b[A\n",
      "Different topics:   3%|█▋                                                             | 12/442 [00:01<01:40,  4.27it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                | 0/127 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  16%|████████▌                                             | 20/127 [00:00<00:00, 191.74it/s]\u001b[A\n",
      "Going through paragraphs:  31%|█████████████████                                     | 40/127 [00:00<00:00, 194.22it/s]\u001b[A\n",
      "Going through paragraphs:  47%|█████████████████████████▌                            | 60/127 [00:00<00:00, 178.33it/s]\u001b[A\n",
      "Going through paragraphs:  61%|█████████████████████████████████▏                    | 78/127 [00:00<00:00, 176.84it/s]\u001b[A\n",
      "Going through paragraphs:  76%|█████████████████████████████████████████▏            | 97/127 [00:00<00:00, 177.73it/s]\u001b[A\n",
      "Going through paragraphs:  95%|██████████████████████████████████████████████████▍  | 121/127 [00:00<00:00, 197.28it/s]\u001b[A\n",
      "Different topics:   3%|█▊                                                             | 13/442 [00:02<02:33,  2.80it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  33%|██████████████████▎                                    | 25/75 [00:00<00:00, 248.91it/s]\u001b[A\n",
      "Going through paragraphs:  67%|████████████████████████████████████▋                  | 50/75 [00:00<00:00, 210.08it/s]\u001b[A\n",
      "Going through paragraphs:  96%|████████████████████████████████████████████████████▊  | 72/75 [00:00<00:00, 211.68it/s]\u001b[A\n",
      "Different topics:   3%|█▉                                                             | 14/442 [00:02<02:31,  2.82it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/74 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  22%|███████████▉                                           | 16/74 [00:00<00:00, 151.89it/s]\u001b[A\n",
      "Going through paragraphs:  43%|███████████████████████▊                               | 32/74 [00:00<00:00, 140.85it/s]\u001b[A\n",
      "Going through paragraphs:  65%|███████████████████████████████████▋                   | 48/74 [00:00<00:00, 147.22it/s]\u001b[A\n",
      "Going through paragraphs:  85%|██████████████████████████████████████████████▊        | 63/74 [00:00<00:00, 147.95it/s]\u001b[A\n",
      "Different topics:   3%|██▏                                                            | 15/442 [00:03<02:47,  2.55it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                                                       \u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Different topics:   4%|██▍                                                            | 17/442 [00:03<01:48,  3.93it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  56%|███████████████████████████████                        | 22/39 [00:00<00:00, 219.11it/s]\u001b[A\n",
      "Different topics:   4%|██▌                                                            | 18/442 [00:03<01:40,  4.21it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/36 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  86%|███████████████████████████████████████████████▎       | 31/36 [00:00<00:00, 307.55it/s]\u001b[A\n",
      "Different topics:   4%|██▋                                                            | 19/442 [00:03<01:28,  4.76it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/77 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  60%|████████████████████████████████▊                      | 46/77 [00:00<00:00, 456.35it/s]\u001b[A\n",
      "Different topics:   5%|██▊                                                            | 20/442 [00:04<01:25,  4.96it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/26 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  96%|████████████████████████████████████████████████████▉  | 25/26 [00:00<00:00, 249.05it/s]\u001b[A\n",
      "Different topics:   5%|██▉                                                            | 21/442 [00:04<01:14,  5.63it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/21 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                                                       \u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "Different topics:   5%|███▎                                                           | 23/442 [00:04<00:57,  7.34it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/46 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  50%|███████████████████████████▌                           | 23/46 [00:00<00:00, 229.36it/s]\u001b[A\n",
      "Different topics:   5%|███▍                                                           | 24/442 [00:04<01:03,  6.57it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/45 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  76%|█████████████████████████████████████████▌             | 34/45 [00:00<00:00, 332.45it/s]\u001b[A\n",
      "Different topics:   6%|███▌                                                           | 25/442 [00:04<01:01,  6.81it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/21 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                                                       \u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/57 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  68%|█████████████████████████████████████▋                 | 39/57 [00:00<00:00, 385.20it/s]\u001b[A\n",
      "Different topics:   6%|███▊                                                           | 27/442 [00:04<00:55,  7.43it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  83%|█████████████████████████████████████████████▌         | 24/29 [00:00<00:00, 239.22it/s]\u001b[A\n",
      "Different topics:   6%|███▉                                                           | 28/442 [00:05<00:54,  7.64it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/38 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  29%|███████████████▉                                       | 11/38 [00:00<00:00, 107.19it/s]\u001b[A\n",
      "Going through paragraphs:  71%|███████████████████████████████████████                | 27/38 [00:00<00:00, 137.06it/s]\u001b[A\n",
      "Different topics:   7%|████▏                                                          | 29/442 [00:05<01:08,  6.04it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/38 [00:00<?, ?it/s]\u001b[A\n",
      "Different topics:   7%|████▎                                                          | 30/442 [00:05<01:01,  6.65it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  96%|████████████████████████████████████████████████████▊  | 24/25 [00:00<00:00, 237.88it/s]\u001b[A\n",
      "Different topics:   7%|████▍                                                          | 31/442 [00:05<00:57,  7.12it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/31 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  65%|███████████████████████████████████▍                   | 20/31 [00:00<00:00, 193.50it/s]\u001b[A\n",
      "Different topics:   7%|████▌                                                          | 32/442 [00:05<01:02,  6.57it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/86 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  26%|██████████████                                         | 22/86 [00:00<00:00, 213.83it/s]\u001b[A\n",
      "Going through paragraphs:  56%|██████████████████████████████▋                        | 48/86 [00:00<00:00, 236.73it/s]\u001b[A\n",
      "Going through paragraphs:  91%|█████████████████████████████████████████████████▉     | 78/86 [00:00<00:00, 261.92it/s]\u001b[A\n",
      "Different topics:   7%|████▋                                                          | 33/442 [00:06<01:26,  4.75it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/81 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  22%|████████████▏                                          | 18/81 [00:00<00:00, 174.80it/s]\u001b[A\n",
      "Going through paragraphs:  44%|████████████████████████▍                              | 36/81 [00:00<00:00, 164.46it/s]\u001b[A\n",
      "Going through paragraphs:  65%|███████████████████████████████████▉                   | 53/81 [00:00<00:00, 162.06it/s]\u001b[A\n",
      "Going through paragraphs:  86%|███████████████████████████████████████████████▌       | 70/81 [00:00<00:00, 156.75it/s]\u001b[A\n",
      "Different topics:   8%|████▊                                                          | 34/442 [00:06<02:03,  3.30it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/29 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  69%|█████████████████████████████████████▉                 | 20/29 [00:00<00:00, 197.19it/s]\u001b[A\n",
      "Different topics:   8%|████▉                                                          | 35/442 [00:06<01:44,  3.89it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                                                       \u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/33 [00:00<?, ?it/s]\u001b[A\n",
      "Different topics:   8%|█████▎                                                         | 37/442 [00:06<01:13,  5.50it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  75%|█████████████████████████████████████████▎             | 21/28 [00:00<00:00, 208.32it/s]\u001b[A\n",
      "Different topics:   9%|█████▍                                                         | 38/442 [00:07<01:08,  5.86it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  74%|████████████████████████████████████████▊              | 26/35 [00:00<00:00, 258.01it/s]\u001b[A\n",
      "Different topics:   9%|█████▌                                                         | 39/442 [00:07<01:05,  6.16it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  24%|█████████████▍                                         | 20/82 [00:00<00:00, 198.16it/s]\u001b[A\n",
      "Going through paragraphs:  66%|████████████████████████████████████▏                  | 54/82 [00:00<00:00, 280.78it/s]\u001b[A\n",
      "Different topics:   9%|█████▋                                                         | 40/442 [00:07<01:20,  5.02it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  47%|█████████████████████████▉                             | 16/34 [00:00<00:00, 150.43it/s]\u001b[A\n",
      "Going through paragraphs:  94%|███████████████████████████████████████████████████▊   | 32/34 [00:00<00:00, 150.49it/s]\u001b[A\n",
      "Different topics:   9%|█████▊                                                         | 41/442 [00:07<01:24,  4.76it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Different topics:  10%|█████▉                                                         | 42/442 [00:07<01:11,  5.57it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  24%|█████████████▍                                           | 8/34 [00:00<00:00, 72.39it/s]\u001b[A\n",
      "Going through paragraphs:  76%|██████████████████████████████████████████             | 26/34 [00:00<00:00, 132.36it/s]\u001b[A\n",
      "Different topics:  10%|██████▏                                                        | 43/442 [00:08<01:22,  4.82it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/74 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  19%|██████████▍                                            | 14/74 [00:00<00:00, 132.40it/s]\u001b[A\n",
      "Going through paragraphs:  38%|████████████████████▊                                  | 28/74 [00:00<00:00, 130.89it/s]\u001b[A\n",
      "Going through paragraphs:  57%|███████████████████████████████▏                       | 42/74 [00:00<00:00, 132.70it/s]\u001b[A\n",
      "Going through paragraphs:  76%|█████████████████████████████████████████▌             | 56/74 [00:00<00:00, 133.31it/s]\u001b[A\n",
      "Going through paragraphs:  95%|████████████████████████████████████████████████████   | 70/74 [00:00<00:00, 128.60it/s]\u001b[A\n",
      "Different topics:  10%|██████▎                                                        | 44/442 [00:08<02:05,  3.16it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/90 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  13%|███████▎                                               | 12/90 [00:00<00:00, 119.07it/s]\u001b[A\n",
      "Going through paragraphs:  28%|███████████████▎                                       | 25/90 [00:00<00:00, 121.77it/s]\u001b[A\n",
      "Going through paragraphs:  42%|███████████████████████▏                               | 38/90 [00:00<00:00, 104.71it/s]\u001b[A\n",
      "Going through paragraphs:  56%|██████████████████████████████▌                        | 50/90 [00:00<00:00, 108.50it/s]\u001b[A\n",
      "Going through paragraphs:  69%|█████████████████████████████████████▉                 | 62/90 [00:00<00:00, 108.68it/s]\u001b[A\n",
      "Going through paragraphs:  81%|████████████████████████████████████████████▌          | 73/90 [00:00<00:00, 106.31it/s]\u001b[A\n",
      "Going through paragraphs:  93%|███████████████████████████████████████████████████▎   | 84/90 [00:00<00:00, 103.10it/s]\u001b[A\n",
      "Different topics:  10%|██████▍                                                        | 45/442 [00:09<03:08,  2.10it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/21 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  57%|███████████████████████████████▍                       | 12/21 [00:00<00:00, 118.50it/s]\u001b[A\n",
      "Different topics:  10%|██████▌                                                        | 46/442 [00:09<02:34,  2.56it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/37 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  35%|███████████████████▎                                   | 13/37 [00:00<00:00, 121.76it/s]\u001b[A\n",
      "Going through paragraphs:  70%|██████████████████████████████████████▋                | 26/37 [00:00<00:00, 123.42it/s]\u001b[A\n",
      "Different topics:  11%|██████▋                                                        | 47/442 [00:10<02:24,  2.73it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/82 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  17%|█████████▍                                             | 14/82 [00:00<00:00, 132.94it/s]\u001b[A\n",
      "Going through paragraphs:  34%|██████████████████▊                                    | 28/82 [00:00<00:00, 131.47it/s]\u001b[A\n",
      "Going through paragraphs:  54%|█████████████████████████████▌                         | 44/82 [00:00<00:00, 139.48it/s]\u001b[A\n",
      "Going through paragraphs:  71%|██████████████████████████████████████▉                | 58/82 [00:00<00:00, 132.79it/s]\u001b[A\n",
      "Going through paragraphs:  90%|█████████████████████████████████████████████████▋     | 74/82 [00:00<00:00, 138.10it/s]\u001b[A\n",
      "Different topics:  11%|██████▊                                                        | 48/442 [00:10<02:51,  2.30it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  65%|███████████████████████████████████▌                   | 22/34 [00:00<00:00, 214.83it/s]\u001b[A\n",
      "Different topics:  11%|██████▉                                                        | 49/442 [00:10<02:18,  2.84it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/60 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  38%|█████████████████████                                  | 23/60 [00:00<00:00, 225.00it/s]\u001b[A\n",
      "Going through paragraphs:  77%|██████████████████████████████████████████▏            | 46/60 [00:00<00:00, 220.42it/s]\u001b[A\n",
      "Different topics:  11%|███████▏                                                       | 50/442 [00:11<02:08,  3.06it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/70 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  44%|████████████████████████▎                              | 31/70 [00:00<00:00, 307.54it/s]\u001b[A\n",
      "Different topics:  12%|███████▎                                                       | 51/442 [00:11<01:52,  3.49it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                                                       \u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  35%|███████████████████▍                                   | 12/34 [00:00<00:00, 117.48it/s]\u001b[A\n",
      "Going through paragraphs:  71%|██████████████████████████████████████▊                | 24/34 [00:00<00:00, 113.17it/s]\u001b[A\n",
      "Different topics:  12%|███████▌                                                       | 53/442 [00:11<01:32,  4.21it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  45%|█████████████████████████▍                              | 10/22 [00:00<00:00, 94.56it/s]\u001b[A\n",
      "Different topics:  12%|███████▋                                                       | 54/442 [00:11<01:29,  4.32it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  32%|██████████████████▏                                      | 8/25 [00:00<00:00, 79.36it/s]\u001b[A\n",
      "Going through paragraphs:  88%|████████████████████████████████████████████████▍      | 22/25 [00:00<00:00, 112.68it/s]\u001b[A\n",
      "Different topics:  12%|███████▊                                                       | 55/442 [00:12<01:30,  4.26it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/80 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  14%|███████▌                                               | 11/80 [00:00<00:00, 104.49it/s]\u001b[A\n",
      "Going through paragraphs:  36%|███████████████████▉                                   | 29/80 [00:00<00:00, 147.29it/s]\u001b[A\n",
      "Going through paragraphs:  56%|██████████████████████████████▉                        | 45/80 [00:00<00:00, 151.46it/s]\u001b[A\n",
      "Going through paragraphs:  76%|█████████████████████████████████████████▉             | 61/80 [00:00<00:00, 131.33it/s]\u001b[A\n",
      "Going through paragraphs:  94%|███████████████████████████████████████████████████▌   | 75/80 [00:00<00:00, 110.41it/s]\u001b[A\n",
      "Different topics:  13%|███████▉                                                       | 56/442 [00:12<02:17,  2.81it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/79 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  11%|██████▍                                                  | 9/79 [00:00<00:00, 83.33it/s]\u001b[A\n",
      "Going through paragraphs:  25%|██████████████▏                                         | 20/79 [00:00<00:00, 94.46it/s]\u001b[A\n",
      "Going through paragraphs:  38%|█████████████████████▎                                  | 30/79 [00:00<00:00, 78.02it/s]\u001b[A\n",
      "Going through paragraphs:  49%|███████████████████████████▋                            | 39/79 [00:00<00:00, 80.99it/s]\u001b[A\n",
      "Going through paragraphs:  61%|██████████████████████████████████                      | 48/79 [00:00<00:00, 81.28it/s]\u001b[A\n",
      "Going through paragraphs:  72%|████████████████████████████████████████▍               | 57/79 [00:00<00:00, 78.88it/s]\u001b[A\n",
      "Going through paragraphs:  84%|██████████████████████████████████████████████▊         | 66/79 [00:00<00:00, 81.06it/s]\u001b[A\n",
      "Going through paragraphs:  95%|█████████████████████████████████████████████████████▏  | 75/79 [00:00<00:00, 81.24it/s]\u001b[A\n",
      "Different topics:  13%|████████                                                       | 57/442 [00:13<03:24,  1.88it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/42 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  21%|████████████▏                                            | 9/42 [00:00<00:00, 81.20it/s]\u001b[A\n",
      "Going through paragraphs:  43%|████████████████████████                                | 18/42 [00:00<00:00, 76.70it/s]\u001b[A\n",
      "Going through paragraphs:  62%|██████████████████████████████████▋                     | 26/42 [00:00<00:00, 73.65it/s]\u001b[A\n",
      "Going through paragraphs:  83%|██████████████████████████████████████████████▋         | 35/42 [00:00<00:00, 77.77it/s]\u001b[A\n",
      "Different topics:  13%|████████▎                                                      | 58/442 [00:14<03:25,  1.87it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/33 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  33%|██████████████████▋                                     | 11/33 [00:00<00:00, 99.16it/s]\u001b[A\n",
      "Going through paragraphs:  64%|███████████████████████████████████▋                    | 21/33 [00:00<00:00, 97.55it/s]\u001b[A\n",
      "Going through paragraphs:  97%|█████████████████████████████████████████████████████▎ | 32/33 [00:00<00:00, 101.58it/s]\u001b[A\n",
      "Different topics:  13%|████████▍                                                      | 59/442 [00:14<03:03,  2.09it/s]\u001b[A\n",
      "Going through paragraphs:   0%|                                                                 | 0/32 [00:00<?, ?it/s]\u001b[A\n",
      "Going through paragraphs:  34%|██████████████████▉                                    | 11/32 [00:00<00:00, 104.99it/s]\u001b[A\n",
      "Going through paragraphs:  72%|████████████████████████████████████████▎               | 23/32 [00:09<00:04,  2.14it/s]\u001b[A\n",
      "Going through paragraphs:  72%|████████████████████████████████████████▎               | 23/32 [00:25<00:04,  2.14it/s]\u001b[A\n",
      "Going through paragraphs:  78%|███████████████████████████████████████████▊            | 25/32 [00:25<00:10,  1.45s/it]\u001b[A\n",
      "Different topics:  13%|████████▍                                                      | 59/442 [00:46<05:04,  1.26it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m get_output_models(\u001b[38;5;28mid\u001b[39m, question, context, \u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     29\u001b[0m     final_row[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mget_output_models\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNew_Passage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     31\u001b[0m     final_row[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#print(final_row)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m, in \u001b[0;36mget_output_models\u001b[1;34m(id, question, context, is_pu)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(models)):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m         preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m(labels[i] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malbert\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     15\u001b[0m             row[tag \u001b[38;5;241m+\u001b[39m  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m labels[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m answer span\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:250\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args_parser(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:1043\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:1065\u001b[0m, in \u001b[0;36mChunkPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1063\u001b[0m all_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params):\n\u001b[1;32m-> 1065\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1066\u001b[0m     all_outputs\u001b[38;5;241m.\u001b[39mappend(model_outputs)\n\u001b[0;32m   1067\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(all_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py:959\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m    958\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 959\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m    960\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:365\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline._forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    363\u001b[0m example \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    364\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m {k: inputs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mmodel_input_names}\n\u001b[1;32m--> 365\u001b[0m start, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: start, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m: end, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample\u001b[39m\u001b[38;5;124m\"\u001b[39m: example, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1513\u001b[0m, in \u001b[0;36mRobertaForQuestionAnswering.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1509\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1511\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1513\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1514\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1525\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1527\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs(sequence_output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:848\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    839\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    841\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    842\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    843\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    846\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    847\u001b[0m )\n\u001b[1;32m--> 848\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    861\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    515\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    516\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    517\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    521\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    522\u001b[0m     )\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 524\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    534\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:409\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    399\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    406\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    408\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:336\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    328\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    334\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    335\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 336\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    346\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:223\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(hidden_states))\n\u001b[1;32m--> 223\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    225\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder:\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;66;03m# Further calls to cross_attention layer can then reuse all cross-attention\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;66;03m# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\u001b[39;00m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# if encoder bi-directional self-attention `past_key_value` is always `None`\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#this is the dataframe that will be converted to pandas and then to csv\n",
    "#final = []\n",
    "#counter = 0\n",
    "#loop through all different data titles/topics associated!\n",
    "fh = open('output.txt', 'w')\n",
    "#loop through all different data titles/topics associated!\n",
    "for i in tqdm(range(len(data)), desc=\"Different topics: \", file=fh):\n",
    "    #get number of paragraphs per topic\n",
    "    length = len(data[i]['paragraphs'])\n",
    "    #get list of all paragraphs for later use\n",
    "    paragraphs = data[i]['paragraphs']\n",
    "    #loop through each paragraph\n",
    "    for j in tqdm(range(len(paragraphs)), leave=False, desc=\"Going through paragraphs: \"):\n",
    "        #get all of the questions associated per paragraph\n",
    "        questions = paragraphs[j]['qas']\n",
    "        #get the context paragraph value\n",
    "        context = paragraphs[j]['context']\n",
    "        #loop through all questions in each topic paragraph\n",
    "        for k in range(0, len(questions)):\n",
    "            if(questions[k]['is_impossible'] == False):\n",
    "                #get the question id\n",
    "                id = questions[k]['id']\n",
    "                if not any(d['Question_Id'] == id for d in final):\n",
    "                    #get question value\n",
    "                    question = questions[k]['question']\n",
    "                    answer = questions[k][\"answers\"][0][\"text\"]\n",
    "                    relevant_pot_qs = samples.loc[samples['Question_Id'] == id]\n",
    "                    for index, row in relevant_pot_qs.iterrows():\n",
    "                        frow = originals.loc[originals['Question_Id'] == id]\n",
    "                        final_row = frow.to_dict('records')[0]\n",
    "                        #final_row = {\"Question_ID\": id,  \"Context\": context, \"Original_Question\": question, \"Original_Answer\": answer, \"Modified_Question\": row[\"Modified_Question\"]}\n",
    "                        #for k, v in get_og_ans(id).items():\n",
    "                        #    final_row[k] = v\n",
    "                        for k, v in get_output_models(id, str(row[\"Modified_Question\"]), context).items():\n",
    "                            final_row[k] = v\n",
    "                        #print(final_row)\n",
    "                        final.append(final_row)\n",
    "                        append_row(final_row)\n",
    "                        save_csv(final)\n",
    "                else:\n",
    "                    used_id = [d for d in final if d.get('Question_Id') == id]\n",
    "                    length = len(used_id)\n",
    "                    used_originals = originals.loc[originals['Question_Id'] == id]\n",
    "                    olen = used_originals.shape[0]\n",
    "                    if(length < olen):\n",
    "                        final = final.loc[final['Question_Id'] != id]\n",
    "                        #get question value\n",
    "                        question = questions[k]['question']\n",
    "                        answer = questions[k][\"answers\"][0][\"text\"]\n",
    "                        relevant_pot_qs = samples.loc[samples['Question_Id'] == id]\n",
    "                        for index, row in relevant_pot_qs.iterrows():\n",
    "                            frow = originals.loc[originals['Question_Id'] == id]\n",
    "                            final_row = frow.to_dict('records')[0]\n",
    "                            #final_row = {\"Question_ID\": id,  \"Context\": context, \"Original_Question\": question, \"Original_Answer\": answer, \"Modified_Question\": row[\"Modified_Question\"]}\n",
    "                            #for k, v in get_og_ans(id).items():\n",
    "                            #    final_row[k] = v\n",
    "                            for k, v in get_output_models(id, str(row[\"Modified_Question\"]), context).items():\n",
    "                                final_row[k] = v\n",
    "                            #print(final_row)\n",
    "                            final.append(final_row)\n",
    "                            append_row(final_row)\n",
    "                            save_csv(final)\n",
    "                        \n",
    "                        \n",
    "                    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a0ee7-eeac-467e-96e3-210ac9c7a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551e8f9f-fda5-4b1a-af92-c00369270466",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c868a989-650a-4701-a992-003a574a4b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"Task13P-U-train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535ed35e-6ca8-40db-96a3-166159fa37fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
